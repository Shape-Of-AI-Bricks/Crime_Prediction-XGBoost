# -*- coding: utf-8 -*-
"""Crime_Prediction-XGBoost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nfcCQ3WDmHvLlYIHI8Cz_7zoE-rBl3gW
"""

#XGBoost is a robust machine-learning algorithm that can help you understand your data and make better decisions. XGBoost is an implementation of gradient-boosting decision trees. It has been used by data scientists and researchers worldwide to optimize their machine-learning models

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder

!git clone https://github.com/abhayshanbhag2003/Crime-Prediction-XGBoost.git

df=pd.read_csv("/content/Crime-Prediction-XGBoost/train.csv")
df_test=pd.read_csv("/content/Crime-Prediction-XGBoost/train.csv")

df.head()

df.isnull().sum()

df.info()

unique_counts = df.nunique()

# Write the number of unique values for each column
for column, count in unique_counts.iteritems():
    print(f"{column}\t': {count}")

df['Type of Homicide'].value_counts() #lesser value hence remove this column

df['Ethnicity'].value_counts()

unique_counts = df.nunique()

# Write the number of unique values for each column
for column, count in unique_counts.iteritems():
    print(f"{column}\t': {count}")

#@title Feature engineering
df['total_amt'] = df['Amount (VVCA)'] + df['Amount (FINE)'] + df['Amount (REST)'] - df['Amount (FSUS)'] # FSUS in suspended fine, hence substracted
df["Total Sentence"] = df["Sentence Imposed"].fillna(0) - df["Sentence Suspended"].fillna(0) + df["Sentence to Serve"].fillna(0) + df["Sentence on Probation"].fillna(0)

#@title Dropping unwanted columns
df = df.drop(['RID','Case ID','Offender ID'],axis=1)
df_test = df_test.drop(['RID','Case ID','Offender ID'],axis=1)
df.drop([ 'Amount (VVCA)', 'Amount (FINE)', 'Amount (REST)', 'Amount (FSUS)'], axis=1, inplace=True)
df = df.drop(["Sentence Imposed", "Sentence Suspended", "Sentence to Serve", "Sentence on Probation"], axis=1)

df.isnull().sum()

df['Age Range'].value_counts()
# here its well distributed

df['No. of Charges'].value_counts()

df['total_amt'] = df['total_amt'].fillna(0) # usually no charges =0 charges :)
df['No. of Charges'] = df['No. of Charges'].fillna(0) #similarly for num of charges

#other columns can be replaced by mean
for col in df.columns:
    if df[col].isna().any():
        if(df[col].dtype!='O'):
          df[col].fillna(df[col].mean(), inplace=True)# for numerical columns->replace with mean
        else:
          df[col].fillna(df[col].mode()[0],inplace=True)# for cateorical columns-->replace by mode

df.isnull().sum()
#removed all null values

#@title Data Visualisation
import matplotlib.pyplot as plt

excluded_columns = ['No. of Charges', 'total_amt', 'Total Sentence']


variables = [col for col in df.columns if col not in excluded_columns]


for var in variables :
    plt.figure(figsize=(10, 6))
    grouped_data = df.groupby([var, 'Type of Offense']).size().unstack()
    grouped_data.plot(kind='bar')
    plt.xlabel(var)
    plt.ylabel('Count')
    plt.title(f'Bar Plot of {var} with respect to Type of Offense')
    plt.legend()
    plt.show()

plt.subplots(figsize=(13,13))
sns.heatmap(df.corr(), annot=True)
plt.show()

#@title Encoding
#label Encoding --->all object dtype to int or float
to_encode = [col for col in df.columns if df[col].dtype == 'O']
encs = {}

for col in to_encode:
    enc = LabelEncoder()
    df[col] = enc.fit_transform(df[col])
    encs[col] = enc

df.info()

df.head()

#@title Splitting Dataset - Train ,CV,Test

from sklearn.model_selection import train_test_split
Y=df['Type of Offense']
X = df.drop('Type of Offense', axis=1)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)


x_tr, x_cv, y_tr, y_cv = train_test_split(x_train, y_train, test_size=0.1, random_state=42)

print("Train set rows :",x_tr.shape[0])
print("CV set rows :",x_cv.shape[0])
print("Test set rows :",x_test.shape[0])

from xgboost.sklearn import XGBRanker
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold  # Optional, if you want to specify the number of folds
from sklearn.model_selection import GridSearchCV

lr = LogisticRegression(max_iter=2048)
lr.fit(x_tr,y_tr)
y_pred_lr=lr.predict(x_cv)
print("Accuracy:", accuracy_score(y_cv, y_pred_lr))

cross_val_scores = cross_val_score(lr, x_train, y_train, cv=5)
print(f" Cross Validation Score: :{cross_val_scores}")

#@title XGBoost
import xgboost as xgb
xgb_clf_base = xgb.XGBClassifier()
xgb_clf_base.fit(x_tr, y_tr)
y_pred_xgb = xgb_clf_base.predict(x_cv)
accuracy = accuracy_score(y_cv, y_pred_xgb)
print(f"Accuracy: {accuracy}")

cross_val_scores = cross_val_score(xgb_clf_base, x_train, y_train, cv=5)
print(f" Cross Validation  Score before Tuning: :{cross_val_scores}")

#@title HyperTuning XGBoost
#initial parameters
xgb1 = XGBClassifier(
 learning_rate =0.1,
 n_estimators=1000,
 max_depth=5,
 min_child_weight=1,
 gamma=0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'multi:softmax',# as we want multiclass classification
 nthread=4,
 scale_pos_weight=1,
 seed=27)


xgb1.fit(x_tr, y_tr)
y_pred_xgb = xgb1.predict(x_cv)

accuracy = accuracy_score(y_cv, y_pred_xgb)
print(f"Accuracy: {accuracy}")

cross_val_scores = cross_val_score(xgb1, x_train, y_train, cv=5)
print(f" Cross Validation  Score before Tuning: :{cross_val_scores}")

param_grid = {
    'n_estimators': [50,100, 200, 300],
    'max_depth': [5, 10, 20, None],

}

xgb_clf =XGBClassifier(random_state=42)


grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, n_jobs=-1)

grid_search.fit(x_cv, y_cv)

print("Best parameters: ", grid_search.best_params_)

"""n_estimater is tuned=100.
Now lets tune lr and child weight
"""

param_grid = {

 'learning_rate' :[0.1,0.05,0.01],
  'min_child_weight':[0.10,1,10,20]
}

xgb_clf =XGBClassifier(random_state=42,max_depth=None,n_estimators=100)

grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, n_jobs=-1)

grid_search.fit(x_cv, y_cv)

print("Best parameters: ", grid_search.best_params_)

param_grid = {

 'subsample':[0.6,0.8,0.9],
 'colsample_bytree':[0.6,0.9,0.1]
}

xgb_clf =XGBClassifier(random_state=42,max_depth=None,n_estimators=100)

grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, n_jobs=-1)

grid_search.fit(x_cv, y_cv)

print("Best parameters: ", grid_search.best_params_)

"""Above I tried to tune *2 parametrs* at a time . Below im tuning *multiple parameters*.Though this took longer,model performance improved.
**NOTE**: Tuning together gives different values for each parameter
"""

param_grid = {
    'learning_rate' :[0.1,0.05,0.01],
  'min_child_weight':[0.10,1,10,20],
 'subsample':[0.6,0.8,0.9],
 'colsample_bytree':[0.6,0.9,0.1],

}

xgb_clf =XGBClassifier(random_state=42,max_depth=None,n_estimators=100)

grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, n_jobs=-1)

grid_search.fit(x_cv, y_cv)

print("Best parameters: ", grid_search.best_params_)

"""now that 6 parameteers are tuned lets see CV scores after tuning"""

xgb_clf_final =XGBClassifier(random_state=42,max_depth=None,n_estimators=100,learning_rate=0.1,min_child_weight=0.1,subsample=0.6,colsample_bytree=0.9)
xgb_clf_final.fit(x_tr,y_tr)
y_pred = xgb_clf_final.predict(x_cv)

print("Accuracy:", accuracy_score(y_cv, y_pred))

cross_val_scores = cross_val_score(xgb_clf_final, x_train, y_train, cv=5)
print(f" Cross Validation Score after tuning :{cross_val_scores}")

"""**Note the improvement in CV Score**(0.70-->0.724)"""

#@title Random Forest
rf = RandomForestClassifier(n_estimators=2000)
rf.fit(x_tr, y_tr)
y_pred = rf.predict(x_cv)

print("Accuracy:", accuracy_score(y_cv, y_pred))

cross_val_scores = cross_val_score(rf, x_train, y_train, cv=5)
print(f" Cross Validation Score before Tuning :{cross_val_scores}")

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 20, None]
}

rf = RandomForestClassifier(random_state=42)

grid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)
grid_search.fit(x_cv, y_cv)

print(grid_search.best_params_)

rf = RandomForestClassifier(random_state=42,max_depth=20,n_estimators=200)

rf.fit(x_train,y_train)
cross_val_scores = cross_val_score(rf, x_train, y_train, cv=5)
print(f" Cross Validation Score after  tuning: :{cross_val_scores}")

"""CV improved here too even though only a 2 parameters were
 tuned.
"""

#@title Comparing Test Results
print("----LR-------")
y_pred_test=lr.predict(x_test)
print("Accuracy:", accuracy_score(y_test, y_pred_test))
print("precision :" ,precision_score(y_test, y_pred_test,average='weighted'))
print( "recall :",recall_score(y_test, y_pred_test,average='weighted'))
print( "F1 Score :",f1_score(y_test, y_pred_test,average='weighted'))
print("\n")

print("-RandomForest-")
y_pred_test=rf.predict(x_test)
print("Accuracy:", accuracy_score(y_test, y_pred_test))
print("precision :" ,precision_score(y_test, y_pred_test,average='weighted'))
print( "recall :",recall_score(y_test, y_pred_test,average='weighted'))
print( "F1 Score :",f1_score(y_test, y_pred_test,average='weighted'))
print("\n")

print("--XGBoost_Base--")
y_pred_test=xgb1.predict(x_test)
print("Accuracy:", accuracy_score(y_test, y_pred_test))
print("precision :" ,precision_score(y_test, y_pred_test,average='weighted'))
print( "recall :",recall_score(y_test, y_pred_test,average='weighted'))
print( "F1 Score :",f1_score(y_test, y_pred_test,average='weighted'))
print("\n")


print("--XGBoost_Tuned---")
y_pred_test=xgb_clf_final.predict(x_test)
print("Accuracy:", accuracy_score(y_test, y_pred_test))
print("precision :" ,precision_score(y_test, y_pred_test,average='weighted'))
print( "recall :",recall_score(y_test, y_pred_test,average='weighted'))
print( "F1 Score :",f1_score(y_test, y_pred_test,average='weighted'))